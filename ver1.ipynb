{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ver1.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOuqYa4iZcYsz0toGcyjSNj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Nguyencongdat1997/A.n.D/blob/master/ver1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Ic5AYGvtJKn"
      },
      "source": [
        "#Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KzwA48VbWKM",
        "outputId": "36b49166-4218-4e83-bad8-fe038e580fbb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# example of loading the mnist dataset\n",
        "from keras.datasets import mnist\n",
        "from matplotlib import pyplot\n",
        "# load dataset\n",
        "(trainX, trainy), (testX, testy) = mnist.load_data()\n",
        "# summarize loaded dataset\n",
        "print('Train: X=%s, y=%s' % (trainX.shape, trainy.shape))\n",
        "print('Test: X=%s, y=%s' % (testX.shape, testy.shape))\n",
        "# plot first few images\n",
        "# for i in range(9):\n",
        "# \t# define subplot\n",
        "# \tpyplot.subplot(330 + 1 + i)\n",
        "# \t# plot raw pixel data\n",
        "# \tpyplot.imshow(trainX[i], cmap=pyplot.get_cmap('gray'))\n",
        "# # show the figure\n",
        "# pyplot.show()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train: X=(60000, 28, 28), y=(60000,)\n",
            "Test: X=(10000, 28, 28), y=(10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzR7iT8HbsjG"
      },
      "source": [
        "#https://github.com/enggen/Deep-Learning-Coursera/blob/master/Convolutional%20Neural%20Networks/Week1/Convolution%20model%20-%20Step%20by%20Step%20-%20v2.ipynb\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "\n",
        "np.random.seed(1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSpKxZ2JcJem"
      },
      "source": [
        "#Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNv8LeR-bzRZ"
      },
      "source": [
        "# def sigmoid(Z):\n",
        "#     A = 1/(1+np.exp(-Z))\n",
        "#     cache = Z\n",
        "#     return A, cache\n",
        "\n",
        "# def softmax(self,z):\n",
        "#     # implement the softmax function\n",
        "#     return 1/sum(np.exp(z)) * np.exp(z)\n",
        "\n",
        "def sigmoid(x, derivative=False):\n",
        "    if derivative:\n",
        "        return (np.exp(-x))/((np.exp(-x)+1)**2)\n",
        "    return 1/(1 + np.exp(-x))\n",
        "\n",
        "def softmax(x, derivative=False):\n",
        "    # Numerically stable with large exponentials\n",
        "    exps = np.exp(x - x.max())\n",
        "    if derivative:\n",
        "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "    return exps / np.sum(exps, axis=0)\n",
        "\n",
        "def relu(Z):\n",
        "    A = np.maximum(0,Z)\n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "def tanh(Z):\n",
        "    A = (np.exp(Z)-np.exp(-Z))/(np.exp(Z)+np.exp(-Z))\n",
        "    cache = Z \n",
        "    return A, cache\n",
        "\n",
        "def relu_backward(dA, Z):\n",
        "    dZ = np.array(dA, copy=True) # convert dz to a correct object.    \n",
        "    dZ[Z <= 0] = 0 # When z <= 0, should set dz to 0 . \n",
        "    return dZ\n",
        "\n",
        "def sigmoid_backward(dA, Z):\n",
        "    s = 1/(1+np.exp(-Z))\n",
        "    dZ = dA * s * (1-s)\n",
        "    return dZ\n",
        "\n",
        "def tanh_backward(dA, Z):\n",
        "    a, cache = tanh(Z)\n",
        "    dZ = dA * (1-a**2)\n",
        "    return dZ\n",
        "\n",
        "class LinearLayer:\n",
        "    def __init__(self, W, b, activation=\"relu\"):\n",
        "        self.W= W\n",
        "        #self.b= b\n",
        "        \n",
        "        self.stored_A_prev = None\n",
        "        self.stored_A = None\n",
        "        self.stored_Z = None\n",
        "        \n",
        "        self.type = \"linear\"\n",
        "        self.activation = activation\n",
        "\n",
        "    def udpate_weight(self, dW, db, learning_rate):\n",
        "        self.W = self.W - learning_rate * dW\n",
        "        #self.b = self.W - learning_rate * db\n",
        "\n",
        "    def forward(self, A_prev, remember_for_backprop=True):\n",
        "        Z = np.dot(self.W, A_prev) # + b\n",
        "        if self.activation == \"sigmoid\":\n",
        "            A = sigmoid(Z)\n",
        "        elif self.activation == \"relu\":\n",
        "            A = relu(Z)\n",
        "        elif self.activation == \"softmax\":\n",
        "            A = softmax(Z)\n",
        "        \n",
        "        self.stored_A_prev = A_prev\n",
        "        self.stored_Z = Z\n",
        "        self.stored_A = A\n",
        "\n",
        "        return A\n",
        "\n",
        "    def backward(self, error):   \n",
        "        '''\n",
        "        neu softmax: error = 2 * (output - y_train) / output.shape[0]\n",
        "        neu sigmoid: error = np.dot(params['W_l+1'].T, error) *\n",
        "\n",
        "        '''\n",
        "        if self.activation == \"sigmoid\":\n",
        "            error = error* sigmoid(self.stored_Z, derivative=True)\n",
        "            change_w = np.outer(error, self.stored_A_prev)\n",
        "        elif self.activation == \"relu\":\n",
        "            pass\n",
        "        elif self.activation == \"softmax\":\n",
        "            error = error* sigmoid(self.stored_Z, derivative=True)\n",
        "            change_w = np.outer(error, self.stored_A_prev)\n",
        "        return error, change_w\n",
        "    "
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A22ommmicOaT"
      },
      "source": [
        "#Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB5uw8eAcNp7"
      },
      "source": [
        "class Model:\n",
        "    def __init__(self, x=784, h1=300, h2=300 ,y=10):\n",
        "        W1,b1 =  self.initialize_linear_param(x, h1)\n",
        "        self.layer1 = LinearLayer(W1, b1, activation=\"sigmoid\")  \n",
        "        W2,b2 =  self.initialize_linear_param(h1, h2)\n",
        "        self.layer2 = LinearLayer(W2, b2, activation=\"sigmoid\") \n",
        "        W3,b3 =  self.initialize_linear_param(h2, y)\n",
        "        self.layer3 = LinearLayer(W3, b3, activation=\"softmax\")\n",
        "        \n",
        "\n",
        "    def L_model_forward(self, X):\n",
        "        # input layer activations becomes sample\n",
        "        a0 = X.reshape((784,1))\n",
        "\n",
        "        # input layer to hidden layer 1\n",
        "        # params['Z1'] = np.dot(params[\"W1\"], params['A0'])\n",
        "        # params['A1'] = self.sigmoid(params['Z1'])\n",
        "        a1 = self.layer1.forward(a0)\n",
        "\n",
        "        # hidden layer 1 to hidden layer 2\n",
        "        # params['Z2'] = np.dot(params[\"W2\"], params['A1'])\n",
        "        # params['A2'] = self.sigmoid(params['Z2'])\n",
        "        a2 = self.layer2.forward(a1)\n",
        "\n",
        "        # hidden layer 2 to output layer\n",
        "        # params['Z3'] = np.dot(params[\"W3\"], params['A2'])\n",
        "        # params['A3'] = self.softmax(params['Z3'])\n",
        "        a3 = self.layer3.forward(a2)\n",
        "\n",
        "        return a3      \n",
        "\n",
        "    def initialize_linear_param(self, n_x, n_y):\n",
        "        W = np.random.randn(n_y, n_x) * 0.01\n",
        "        b = np.zeros(shape=(n_y, 1))\n",
        "        return W,b \n",
        "\n",
        "    def L_model_backward(self, y_train, output):\n",
        "        change_w = {}\n",
        "        # Calculate W3 update\n",
        "        error = 2 * (output - y_train) / output.shape[0]\n",
        "        error, change_w['W3'] = self.layer3.backward(error)\n",
        "\n",
        "        # Calculate W2 update\n",
        "        error = np.dot(self.layer3.W.T, error) \n",
        "        error, change_w['W2'] = self.layer2.backward(error)\n",
        "\n",
        "        # Calculate W1 update\n",
        "        error = np.dot(self.layer2.W.T, error) \n",
        "        error, change_w['W1'] = self.layer1.backward(error)\n",
        "\n",
        "        return change_w\n",
        "\n",
        "    def update_network_parameters(self, change_w, lr):\n",
        "        db = 0\n",
        "        self.layer1.udpate_weight(change_w['W1'], db, lr)\n",
        "        self.layer2.udpate_weight(change_w['W2'], db, lr)\n",
        "        self.layer3.udpate_weight(change_w['W3'], db, lr)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWy28dWBtGCl"
      },
      "source": [
        "#Trainer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX4EtMhGsCVV"
      },
      "source": [
        "def compute_accuracy(model, x_val, y_val):\n",
        "        predictions = []\n",
        "        for x, y in zip(x_val, y_val):\n",
        "            output = model.L_model_forward(x)\n",
        "            pred = np.argmax(output)\n",
        "            predictions.append(pred == np.argmax(y))   \n",
        "        return np.mean(predictions)\n",
        "def train(model, x_train, y_train, x_val, y_val, epochs=100, lr=0.0075):\n",
        "    start_time = time.time()\n",
        "    for iteration in range(epochs):\n",
        "        for x,y in zip(x_train, y_train):\n",
        "            output = model.L_model_forward(x)\n",
        "            changes_to_w = model.L_model_backward(y, output)\n",
        "            model.update_network_parameters(changes_to_w, lr)\n",
        "        \n",
        "        accuracy = compute_accuracy(model, x_val, y_val)\n",
        "        print('Epoch: {0}, Time Spent: {1:.2f}s, Accuracy: {2:.2f}%'.format(\n",
        "            iteration+1, time.time() - start_time, accuracy * 100\n",
        "        ))"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8jBjO4G1sw_v",
        "outputId": "74d83b97-6d80-4915-f806-6dad913142e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 561
        }
      },
      "source": [
        "dnn = Model(x=784, h1=128, h2=64, y=10)\n",
        "train(dnn, trainX, trainy, testX, testy)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W1 (128, 784)\n",
            "Epoch: 1, Time Spent: 60.41s, Accuracy: 2.46%\n",
            "Epoch: 2, Time Spent: 121.16s, Accuracy: 23.09%\n",
            "Epoch: 3, Time Spent: 181.93s, Accuracy: 30.06%\n",
            "Epoch: 4, Time Spent: 242.38s, Accuracy: 32.69%\n",
            "Epoch: 5, Time Spent: 304.00s, Accuracy: 34.63%\n",
            "Epoch: 6, Time Spent: 365.21s, Accuracy: 36.33%\n",
            "Epoch: 7, Time Spent: 425.99s, Accuracy: 37.54%\n",
            "Epoch: 8, Time Spent: 486.39s, Accuracy: 38.32%\n",
            "Epoch: 9, Time Spent: 546.66s, Accuracy: 39.24%\n",
            "Epoch: 10, Time Spent: 607.44s, Accuracy: 40.45%\n",
            "Epoch: 11, Time Spent: 668.05s, Accuracy: 41.61%\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-52-262bc098173f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m784\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh1\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtesty\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-51-85e0da80e29f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, x_train, y_train, x_val, y_val, epochs, lr)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m             \u001b[0mchanges_to_w\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL_model_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_network_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchanges_to_w\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-f6195e70ac32>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;31m# params['Z1'] = np.dot(params[\"W1\"], params['A0'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# params['A1'] = self.sigmoid(params['Z1'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0ma1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# hidden layer 1 to hidden layer 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-83be841404f8>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, A_prev, remember_for_backprop)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mremember_for_backprop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_prev\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# + b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"sigmoid\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7v5kqcIP1bP-"
      },
      "source": [
        "Epoch: 1, Time Spent: 60.41s, Accuracy: 2.46%\n",
        "Epoch: 2, Time Spent: 121.16s, Accuracy: 23.09%\n",
        "Epoch: 3, Time Spent: 181.93s, Accuracy: 30.06%\n",
        "Epoch: 4, Time Spent: 242.38s, Accuracy: 32.69%\n",
        "Epoch: 5, Time Spent: 304.00s, Accuracy: 34.63%\n",
        "Epoch: 6, Time Spent: 365.21s, Accuracy: 36.33%\n",
        "Epoch: 7, Time Spent: 425.99s, Accuracy: 37.54%\n",
        "Epoch: 8, Time Spent: 486.39s, Accuracy: 38.32%\n",
        "Epoch: 9, Time Spent: 546.66s, Accuracy: 39.24%\n",
        "Epoch: 10, Time Spent: 607.44s, Accuracy: 40.45%\n",
        "Epoch: 11, Time Spent: 668.05s, Accuracy: 41.61%"
      ]
    }
  ]
}